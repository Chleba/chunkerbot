use clap::{Parser, ValueEnum};
use futures_util::StreamExt;
use reqwest::Url;
use serde::{Deserialize, Serialize};
use serde_json::Value;
use unescape::unescape;

use std::{collections::HashMap, fs, io::Write, sync::Arc, time::Duration};
use text_splitter::{ChunkConfig, TextSplitter};
use tiktoken_rs::cl100k_base;

use langchain_rust::{
    chain::{builder::ConversationalChainBuilder, Chain, ConversationalRetrieverChainBuilder},
    document_loaders::{pdf_extract_loader::PdfExtractLoader, Loader},
    embedding::OllamaEmbedder,
    fmt_message, fmt_template,
    llm::client::{GenerationOptions, Ollama, OllamaClient},
    memory::SimpleMemory,
    message_formatter, output_parsers,
    prompt::HumanMessagePromptTemplate,
    prompt_args,
    schemas::{Document, Message, Retriever},
    template_jinja2,
    vectorstore::{
        qdrant::{Qdrant, StoreBuilder},
        // Retriever, VecStoreOptions, VectorStore,
        VecStoreOptions,
        VectorStore,
    },
};

pub const CONTEXT_CHUNK_STR: &str = "
Jsi asistent pro zpracov√°n√≠ textu. Tv√Ωm √∫kolem je roz≈°√≠≈ôit dan√Ω chunk textu pomoc√≠ kontextu z cel√©ho dokumentu tak, aby byl co nejv√≠ce srozumiteln√Ω a informativn√≠ i p≈ôi samostatn√©m pou≈æit√≠. Doplnƒõn√≠m kontextu zajist√≠≈°, ≈æe chunk obsahuje kl√≠ƒçov√© informace, kter√© mu chyb√≠, a z√°rove≈à z≈Østane struƒçn√Ω a relevantn√≠.

Vstup:

Cel√Ω dokument:
{{document}}  

P≈Øvodn√≠ chunk:
{{input}}  

Po≈æadavky na v√Ωstup:
    Doplnƒõn√≠ kontextu ‚Äì Pokud chunk odkazuje na nejasn√© subjekty, ud√°losti nebo pojmy, dopl≈à je z kontextu cel√©ho dokumentu.
    Konzistence ‚Äì Zachovej styl a terminologii dokumentu.
    Struƒçnost ‚Äì Chunk nesm√≠ b√Ωt p≈ô√≠li≈° dlouh√Ω, ale mƒõl by obsahovat v≈°echny kl√≠ƒçov√© informace.
    Koherence ‚Äì Chunk by mƒõl d√°vat smysl i s√°m o sobƒõ, bez nutnosti ƒç√≠st cel√Ω dokument.

V√Ωstup:
Vr√°t√≠≈° p≈ôeformulovan√Ω chunk s doplnƒõn√Ωm kontextem. Nep≈ôid√°vej ≈æ√°dn√© zbyteƒçn√© informace, kter√© nejsou v dokumentu.
";

#[derive(Copy, Clone, PartialEq, Eq, PartialOrd, Ord, ValueEnum)]
enum Mode {
    Chat,
    Generate,
}

#[derive(Parser)]
#[command(version, about, long_about = None)]
struct Cli {
    // chatting and generating model
    #[arg(short, long, default_value = "gemma3:12b")]
    model: Option<String>,
    // embedding model
    #[arg(short, long, default_value = "paraphrase-multilingual")]
    embed: Option<String>,
    // qdrant db url
    #[arg(long, default_value = "http://localhost:6334")]
    db: Option<String>,
    #[arg(short, long)]
    document: Option<String>,
    #[arg(short, long, default_value = "http://localhost:11434")]
    ollama: Option<String>,
    #[arg(value_enum)]
    mode: Mode,
}

async fn chat(ollama_url: String, model: String, embed: String, db_url: String) {
    // -- llm
    let ollama_client = Arc::new(OllamaClient::from_url(Url::parse(&ollama_url).unwrap()));
    let ollama = Ollama::new(
        ollama_client.clone(),
        &model,
        Some(GenerationOptions::default()),
    );

    let msg_template = template_jinja2!(
        "
    Jsi pokroƒçil√Ω AI asistent, kter√Ω odpov√≠d√° na ot√°zky na z√°kladƒõ poskytnut√©ho kontextu.  
    Tvoje √∫loha je analyzovat poskytnut√© informace a vybrat **pouze ty nejrelevantnƒõj≈°√≠** pro odpovƒõƒè.  

    üìå **Ot√°zka u≈æivatele:**  
    {{question}}

    üìå **Poskytnut√© informace (m≈Ø≈æe obsahovat irelevantn√≠ ƒç√°sti):**  
    {{context}}

    üìå **Instrukce pro odpovƒõƒè:**  
    1. **Pou≈æ√≠vej historii konverzace k udr≈æen√≠ kontextu.** Pokud ot√°zka odkazuje na p≈ôedchoz√≠ ƒç√°st dialogu, zohledni ji.  
    2. **Peƒçlivƒõ vyhodno≈•, kter√© ƒç√°sti poskytnut√©ho textu jsou relevantn√≠.** Nepou≈æ√≠vej irelevantn√≠ informace.  
    3. **Odpovƒõz podrobnƒõ a strukturovanƒõ.** Pokud je to vhodn√©, pou≈æij odstavce, seznamy nebo p≈ô√≠klady.  
    4. **Zahr≈à souvisej√≠c√≠ informace, kter√© mohou b√Ωt u≈æiteƒçn√© pro odpovƒõƒè.**  
    5. **Nevyu≈æ√≠vej ≈æ√°dn√© jin√© znalosti mimo poskytnut√Ω kontext a historii konverzace.**  
    6. **Pokud v poskytnut√Ωch informac√≠ch odpovƒõƒè chyb√≠, p≈ôiznej to, ale nab√≠dni u≈æiteƒçn√© dopl≈àuj√≠c√≠ informace, pokud to d√°v√° smysl.**  

    **Tvoje odpovƒõƒè:**",
    "context",
    "question"
    );

    // let msg_template = template_jinja2!(
    //     "Odpoved na otazku pouze z tohoto textu: {{context}}.
    // Otazka: {{question}}",
    //     "context",
    //     "question"
    // );

    let ollama_embed = OllamaEmbedder::new(
        ollama_client.clone(),
        &embed,
        Some(GenerationOptions::default()),
    );
    let db_client = Qdrant::from_url(&db_url).build().unwrap();
    let vector_store = StoreBuilder::new()
        .recreate_collection(false)
        .embedder(ollama_embed)
        .client(db_client)
        .collection_name("documents")
        .build()
        .await
        .unwrap();

    let prompt = message_formatter![
        fmt_message!(Message::new_system_message("Jsi AI pomocnik ve firme S&W pro strucne odpovedi na dotazy z dodanych documents internich smernic. Odpovidej co nepresneji dle dodaneho kontextu.")),
        fmt_template!(HumanMessagePromptTemplate::new(msg_template))
    ];
    let retviever = langchain_rust::vectorstore::Retriever::new(vector_store, 5)
        .with_options(VecStoreOptions::new().with_score_threshold(0.5));
    let chain = ConversationalRetrieverChainBuilder::new()
        .llm(ollama)
        .rephrase_question(true)
        .memory(SimpleMemory::new().into())
        .retriever(retviever)
        .return_source_documents(true)
        .prompt(prompt)
        .build()
        .expect("Error building ConversationalChain");

    loop {
        // Ask for user input
        println!("\n");
        print!("Query> ");
        std::io::stdout().flush().unwrap();
        let mut query = String::new();
        std::io::stdin().read_line(&mut query).unwrap();

        let query = query.trim(); // Trim input to avoid issues with empty queries
        if query.is_empty() {
            println!("Empty query. Exiting...");
            break;
        }

        let input_variables = prompt_args! {
            "question" => &query,
        };

        if let result = chain.execute(input_variables).await {
            match result {
                Ok(data) => {
                    let output = data["output"].as_str().unwrap();
                    let out_formatted = unescape(output).unwrap();

                    let mut used_docs: Vec<String> = data["source_documents"]
                        .as_array()
                        .unwrap()
                        .iter()
                        .map(|d| {
                            // format!("{} (s:{})", d["metadata"]["path"], d["score"])
                            format!("{}", d["metadata"]["path"])
                            // let mut d_str = d["metadata"]["path"].as_str().unwrap().to_string();
                        })
                        .collect();
                    used_docs.sort();
                    used_docs.dedup();

                    println!("{}", out_formatted);
                    println!("-------\ndocuments:[{}]", used_docs.join(", "));
                }
                Err(e) => {
                    println!("Error: {:?}", e);
                }
            }
        };

        // let mut stream = chain.stream(input_variables).await.unwrap();
        // while let Some(result) = stream.next().await {
        //     match result {
        //         Ok(data) => data.to_stdout().unwrap(),
        //         Err(e) => {
        //             println!("Error: {:?}", e);
        //         }
        //     }
        // }
    }
}

fn get_pdf_files(directory: &str) -> Vec<String> {
    let mut pdf_files = Vec::new();
    if let Ok(entries) = fs::read_dir(directory) {
        for entry in entries.flatten() {
            let path = entry.path();
            if path.is_file() {
                if let Some(extension) = path.extension() {
                    if extension == "pdf" {
                        if let Some(path_str) = path.to_str() {
                            pdf_files.push(path_str.to_string());
                        }
                    }
                }
            }
        }
    }
    pdf_files
}

async fn generate(
    document: String,
    ollama_url: String,
    model: String,
    embed: String,
    db_url: String,
) {
    // -------------------------------------
    // -- VARIABLES
    // let documents = get_pdf_files("./assets");
    // println!("{:?} - documents", documents);

    let documents = vec![document];

    let ollama_client = Arc::new(OllamaClient::from_url(Url::parse(&ollama_url).unwrap()));
    let ollama = Ollama::new(
        ollama_client.clone(),
        &model,
        Some(GenerationOptions::default()),
    );

    let chunk_msg_template = template_jinja2!(CONTEXT_CHUNK_STR, "document", "input");
    let prompt = message_formatter![fmt_template!(HumanMessagePromptTemplate::new(
        chunk_msg_template
    ))];
    let chain = ConversationalChainBuilder::new()
        .llm(ollama.clone())
        .prompt(prompt)
        .build()
        .expect("Error building ConversationalChain");

    for doc_path in documents {
        // -------------------------------------
        // -- documents loader text extractor
        let loader = PdfExtractLoader::from_path(doc_path.clone()).unwrap();
        let doc = loader
            .load()
            .await
            .unwrap()
            .map(|d| d.unwrap())
            .collect::<Vec<_>>()
            .await;
        log::info!("{:?}", doc);

        // -------------------------------------
        // -- spliting into a meaningful chunks
        let mut chunks_vec: Vec<Document> = vec![];
        let mut doc_text: String = "".to_string();

        let tokenizer = cl100k_base().unwrap();
        let max_tokens = 512;
        let chunk_config = ChunkConfig::new(max_tokens).with_sizer(tokenizer);
        let splitter = TextSplitter::new(chunk_config);
        for doc_entry in doc.iter() {
            doc_text += &doc_entry.page_content;
            let chunks = splitter
                .chunks(&doc_entry.page_content)
                .map(|d| Document::new(d))
                .collect::<Vec<_>>();
            chunks_vec.extend(chunks);
        }

        // -------------------------------------
        // -- rephrase document to questions with contextual wrapping
        let mut context_chunks: Vec<Document> = vec![];
        for chunk in chunks_vec.iter() {
            let input_vars = prompt_args! {
                "document" => doc_text,
                "input" => &chunk.page_content,

            };

            println!("----------------------------");
            println!("CHUNK:");
            println!("{:?}", chunk.page_content);
            println!("---\n");

            match chain.invoke(input_vars).await {
                Ok(result) => {
                    println!("RESULT:");
                    println!("{:?}", result);
                    let mut h = HashMap::new();
                    h.insert("path".to_string(), Value::String(doc_path.clone()));
                    let d = Document::new(result).with_metadata(h);
                    context_chunks.push(d);
                }
                Err(e) => panic!("Error invoking LLMChain: {:?}", e),
            }

            // -------------------------------------
            // -- sleep between chunks so poor GPU don't blow up
            // tokio::time::sleep(Duration::from_secs(20)).await;
        }

        // -------------------------------------
        // -- embeddings & vector store
        let db_client = Qdrant::from_url(&db_url).build().unwrap();
        let ollama_embed = OllamaEmbedder::new(
            ollama_client.clone(),
            &embed,
            Some(GenerationOptions::default()),
        );
        let vector_store = StoreBuilder::new()
            .embedder(ollama_embed)
            // .recreate_collection(true)
            .client(db_client)
            .collection_name("documents")
            .build()
            .await
            .unwrap();
        vector_store
            .add_documents(&context_chunks, &VecStoreOptions::default())
            .await
            .unwrap();
    }
}

#[tokio::main]
async fn main() {
    env_logger::init();

    let cli = Cli::parse();
    match cli.mode {
        Mode::Chat => {
            chat(
                cli.ollama.unwrap(),
                cli.model.unwrap(),
                cli.embed.unwrap(),
                cli.db.unwrap(),
            )
            .await;
        }
        Mode::Generate => {
            if cli.document.is_none() {
                println!("Missing document for generating chunks. \nAdd --document [path_to_document] into aruments.");
                return;
            }
            generate(
                cli.document.unwrap(),
                cli.ollama.unwrap(),
                cli.model.unwrap(),
                cli.embed.unwrap(),
                cli.db.unwrap(),
            )
            .await;
        }
    }
}
